# -*- coding: utf-8 -*-
"""ee660_project_tadab_f21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14MY5tU4ZIyTzDEByhuz3csOPaGLObBvm
"""

'''Please pip3 install adapt before running this file'''

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd

from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.base import clone
from sklearn.utils.validation import check_array

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.ensemble import AdaBoostClassifier

from adapt.instance_based import TrAdaBoost
from adapt.base import BaseAdaptEstimator
from scipy.sparse import vstack, issparse

class OwnTrAdaBoost(object):
    def __init__(self,N=10,base_estimator=DecisionTreeClassifier(),score=roc_auc_score):    
        self.N=N
        self.base_estimator=base_estimator
        self.score=score
        self.beta_all = None
        self.estimators=[]
            
    def _calculate_weights(self,weights): 
        weights = weights.ravel()     
        total = np.sum(weights)   
        print(total,np.min(weights),np.max(weights))   
        return np.asarray(weights / total, order='C')      
                    
    def _calculate_error_rate(self,y_true, y_pred, weight):      
        weight = weight.ravel()
        total = np.sum(weight) 
        print(total,np.min(weight),np.max(weight))     
        return np.sum(weight / total * np.abs(y_true ^ y_pred))      
             
    def fit(self,source,target,source_label,target_label):
        source_shape=source.shape[0]
        target_shape=target.shape[0]
        trans_data = np.concatenate((source, target), axis=0)      
        trans_label = np.concatenate((source_label,target_label), axis=0)      
        weights_source = np.ones([source_shape, 1])/source_shape      
        weights_target = np.ones([target_shape, 1])/target_shape
        weights = np.concatenate((weights_source, weights_target), axis=0)
        
        bata = 1 / (1 + np.sqrt(2 * np.log(source_shape) / self.N))    
        self.beta_all = np.zeros([1, self.N])
        result_label = np.ones([source_shape+target_shape, self.N])    

        trans_data = np.asarray(trans_data, order='C')
        trans_label = np.asarray(trans_label, order='C')     
        
        best_round = 0
        score=0
        flag=0
        
        for i in range(self.N):      
            P = self._calculate_weights(weights) 
            est = clone(self.base_estimator).fit(trans_data,trans_label,sample_weight=P.ravel())
            self.estimators.append(est)
            y_preds=est.predict(trans_data)
            result_label[:, i]=y_preds

            y_target_pred=est.predict(target)
            error_rate = self._calculate_error_rate(target_label, y_target_pred,  \
                                              weights[source_shape:source_shape + target_shape, :])  
            if error_rate >= 0.5 or error_rate == 0:      
                self.N = i
                print('early stop! due to error_rate=%.2f'%(error_rate))      
                break       

            self.beta_all[0, i] = error_rate / (1 - error_rate)      
     
            for j in range(target_shape):      
                weights[source_shape + j] = weights[source_shape + j] * \
                np.power(self.beta_all[0, i],(-np.abs(result_label[source_shape + j, i] - target_label[j])))
  
            for j in range(source_shape):      
                weights[j] = weights[j] * np.power(bata,np.abs(result_label[j, i] - source_label[j]))
                
            tp=self.score(target_label,y_target_pred)
            print('The '+str(i)+' rounds score is '+str(tp))

    def _predict_one(self, x):
        """
        Output the hypothesis for a single instance
        :param x: array-like
            target label of a single instance from each iteration in order
        :return: 0 or 1
        """
        x, N = check_array(x, ensure_2d=False), self.N
        # replace 0 by 1 to avoid zero division and remove it from the product
        beta = [self.beta_all[0,t] if self.beta_all[0,t] != 0 else 1 for t in range(int(np.ceil(N/2)), N)]
        cond = np.prod([b ** -x for b in beta]) >= np.prod([b ** -0.5 for b in beta])
        return int(cond)

    def predict(self, x_test):
        y_pred_list = np.array([est.predict(x_test) for est in self.estimators]).T
        y_pred = np.array(list(map(self._predict_one, y_pred_list)))
        return y_pred

# We create here the AUX model which consist in a balanced weighting
# between instances from source and target domains.
class BalancedWeighting(BaseAdaptEstimator):

    def __init__(self, estimator=None, alpha=1., Xt=None, yt=None):
        super().__init__(estimator=estimator, alpha=alpha, Xt=Xt, yt=yt)

    def fit(self, Xs, ys, Xt=None, yt=None, **kwargs):
        Xt, yt = self._get_target_data(Xt, yt)
        if issparse(Xs):
            X = vstack((Xs, Xt))
        else:
            X = np.concatenate((Xs, Xt))
        y = np.concatenate((ys, yt))
        sample_weight = np.ones(X.shape[0])
        sample_weight[Xs.shape[0]:] *= (Xs.shape[0] / Xt.shape[0]) * self.alpha

        self.fit_estimator(X, y, sample_weight=sample_weight)

def implement_own(Xs, ys, Xt, yt, Xtest, ytest, model):
    if model == 1:
        est = DecisionTreeClassifier(max_depth=2)
    elif model == 2:
        est = LinearSVC()
    elif model == 3:
        est = SVC()
    elif model == 4:
        baseline = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=3)
        baseline.fit(Xs,ys)
        ys_pred = baseline.predict(Xs)
        yt_pred = baseline.predict(Xt)
        ytest_pred = baseline.predict(Xtest)
        print('train acc:',accuracy_score(ys,ys_pred))
        print('target acc:',accuracy_score(yt,yt_pred))
        print('target_test acc:',accuracy_score(ytest,ytest_pred))
        return

    clf = OwnTrAdaBoost(N=3, base_estimator=est, score=accuracy_score)
    clf.fit(Xs, Xt, ys, yt)

    ys_pred = clf.predict(Xs)
    yt_pred = clf.predict(Xt)
    ytest_pred = clf.predict(Xtest)
    print('train acc:',accuracy_score(ys,ys_pred))
    print('target acc:',accuracy_score(yt,yt_pred))
    print('target_test acc:',accuracy_score(ytest,ytest_pred))
    return

def implement_adapt(Xs, ys, Xt, yt, model):
    if model == 1:
        est = GaussianProcessClassifier()
    elif model == 2:
        est = LinearSVC()
    elif model == 3:
        est = MLPClassifier()
    elif model == 4:
        LinearDiscriminantAnalysis()
    
    tradaboost_model = TrAdaBoost(est, n_estimators=10, Xt=Xt, yt=yt, random_state=0)
    tradaboost_model.fit(Xs, ys)
    return tradaboost_model.score(Xt, yt)

def implement_all(Xs, ys, Xt, yt, Xtest, ytest):
    names = ["SVM", "SVMt", "AUX", "TrAdaBoost"]
    scores = {k: [] for k in names}

    for state in range(10):
        np.random.seed(state)
        if state == 0:
            print("Xs shape: %s, Xt shape: %s"%(str(Xs.shape), str(Xt.shape)))
        models = [
            LinearSVC(class_weight="balanced"),
            LinearSVC(class_weight="balanced"),
            BalancedWeighting(LinearSVC(class_weight="balanced"), alpha=4., Xt=Xtest, yt=ytest),
            TrAdaBoost(LinearSVC(class_weight="balanced"), n_estimators=100, verbose=0, Xt=Xtest, yt=ytest)
        ]
        for model, name in zip(models, names):
            model.fit(Xs, ys)
            scores[name].append(1-model.score(Xt, yt))

        print("Round %i : %s"%(state, str({k: v[-1] for k, v in scores.items()})))

    error_mu = np.round(pd.DataFrame(pd.DataFrame(scores).mean(0), columns=["Error"]), 3).transpose().astype(str)
    error_std = np.round(pd.DataFrame(pd.DataFrame(scores).std(0), columns=["Error"]), 3).transpose().astype(str)
    # display(error_mu + " (" + error_std + ")")
    print(error_mu, error_std)
    return

def execute():
    datafile = 'flare.dat'
    df = pd.read_csv(datafile, sep="\s+", header=None)

    cat = pd.DataFrame()
    cat[0] = df[0]
    cat[1] = df[1]
    cat[2] = df[11]
    df = df.drop([0,1,11], axis=1)

    cat = cat.apply(LabelEncoder().fit_transform)
    df[0], df[1], classes = cat[0], cat[1], cat[2]

    for i in range(2,11):
        df[i] = df[i].str.replace(',', '').astype(int)

    model = KMeans(n_clusters=2)
    model.fit(df)
    y_pred = pd.DataFrame(model.predict(df))
    clusters = model.fit_predict(df)

    df['y_pred'] = y_pred

    Xt = df[df['y_pred'] == 0]
    yt = classes[df['y_pred']==0]
    Xs = df[df['y_pred'] == 1]
    ys = classes[df['y_pred'] == 1]

    split = int(len(Xt)/2)
    Xtest = Xt[split:]
    ytest = yt[split:]
    Xt = Xt[:split]
    yt = yt[:split]

    ys = ys.reset_index(drop=True)
    yt = yt.reset_index(drop=True)
    ytest = ytest.reset_index(drop=True)

    option = input('''
    Which implementation would you like to choose?
    Enter as follows:
    1 for Custom Implementation
    2 for Python Adapt Implementation
    3 for SVM, SVMt, AUX and TrAdaBoost''')

    if option == 1:
        model = input('''
        Which model would you like to run for Custom Implementation?
        Enter as follows:
        1 for Decision Tree
        2 for Linear SVC
        3 for SVC
        4 for AdaBoost''')
        return implement_own(Xs, ys, Xt, yt, Xtest, ytest, model)

    elif option == 2:
        model = input('''
        Which model would you like to run for Adapt Implementation?
        Enter as follows:
        1 for Gaussian Process Classifier
        2 for Linear SVC
        3 for MLP Classifier
        4 for Linear Disciminant Analysis
        ''')
        return implement_adapt(Xs, ys, Xt, yt, model)

    elif option == 3:
        implement_all(Xs, ys, Xt, yt, Xtest, ytest)